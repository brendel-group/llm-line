models:
  - name: "HuggingFaceFW/ablation-model-the-pile" 
    revision: "step-001000-2BT"
    cls: "hf"
    batch_size: 100
    device: "cuda"
    architecture: "llama"
    dataset: "The Pile"
    dataset_version: "v1"
    dataset_size: "300B"
    tokenizer_name: "EleutherAI/gpt-neox-20b"

  - name: "HuggingFaceFW/ablation-model-the-pile" 
    revision: "step-005000-10BT"
    cls: "hf"
    batch_size: 100
    device: "cuda"
    architecture: "llama"
    dataset: "The Pile"
    dataset_version: "v1"
    dataset_size: "300B"
    tokenizer_name: "EleutherAI/gpt-neox-20b"

  - name: "HuggingFaceFW/ablation-model-the-pile" 
    revision: "step-010000-21BT"
    cls: "hf"
    batch_size: 100
    device: "cuda"
    architecture: "llama"
    dataset: "The Pile"
    dataset_version: "v1"
    dataset_size: "300B"
    tokenizer_name: "EleutherAI/gpt-neox-20b"

  - name: "HuggingFaceFW/ablation-model-the-pile" 
    revision: "step-015000-31BT"
    cls: "hf"
    batch_size: 100
    device: "cuda"
    architecture: "llama"
    dataset: "The Pile"
    dataset_version: "v1"
    dataset_size: "300B"
    tokenizer_name: "EleutherAI/gpt-neox-20b"

  - name: "HuggingFaceFW/ablation-model-the-pile" 
    revision: "step-020000-42BT"
    cls: "hf"
    batch_size: 100
    device: "cuda"
    architecture: "llama"
    dataset: "The Pile"
    dataset_version: "v1"
    dataset_size: "300B"
    tokenizer_name: "EleutherAI/gpt-neox-20b"

  - name: "HuggingFaceFW/ablation-model-the-pile" 
    revision: "step-025000-52BT"
    cls: "hf"
    batch_size: 100
    device: "cuda"
    architecture: "llama"
    dataset: "The Pile"
    dataset_version: "v1"
    dataset_size: "300B"
    tokenizer_name: "EleutherAI/gpt-neox-20b"

  - name: "HuggingFaceFW/ablation-model-the-pile" 
    revision: "step-030000-63BT"
    cls: "hf"
    batch_size: 100
    device: "cuda"
    architecture: "llama"
    dataset: "The Pile"
    dataset_version: "v1"
    dataset_size: "300B"
    tokenizer_name: "EleutherAI/gpt-neox-20b"

  - name: "HuggingFaceFW/ablation-model-the-pile" 
    revision: "step-035000-73BT"
    cls: "hf"
    batch_size: 100
    device: "cuda"
    architecture: "llama"
    dataset: "The Pile"
    dataset_version: "v1"
    dataset_size: "300B"
    tokenizer_name: "EleutherAI/gpt-neox-20b"

  - name: "HuggingFaceFW/ablation-model-the-pile" 
    revision: "step-040000-84BT"
    cls: "hf"
    batch_size: 100
    device: "cuda"
    architecture: "llama"
    dataset: "The Pile"
    dataset_version: "v1"
    dataset_size: "300B"
    tokenizer_name: "EleutherAI/gpt-neox-20b"

  - name: "HuggingFaceFW/ablation-model-the-pile" 
    revision: "step-046000-96BT"
    cls: "hf"
    batch_size: 100
    device: "cuda"
    architecture: "llama"
    dataset: "The Pile"
    dataset_version: "v1"
    dataset_size: "300B"
    tokenizer_name: "EleutherAI/gpt-neox-20b"

  - name: "HuggingFaceFW/ablation-model-the-pile" 
    revision: "step-050000-105BT"
    cls: "hf"
    batch_size: 100
    device: "cuda"
    architecture: "llama"
    dataset: "The Pile"
    dataset_version: "v1"
    dataset_size: "300B"
    tokenizer_name: "EleutherAI/gpt-neox-20b"

  - name: "HuggingFaceFW/ablation-model-the-pile" 
    revision: "step-055000-115BT"
    cls: "hf"
    batch_size: 100
    device: "cuda"
    architecture: "llama"
    dataset: "The Pile"
    dataset_version: "v1"
    dataset_size: "300B"
    tokenizer_name: "EleutherAI/gpt-neox-20b"

  - name: "HuggingFaceFW/ablation-model-the-pile" 
    revision: "step-060000-126BT"
    cls: "hf"
    batch_size: 100
    device: "cuda"
    architecture: "llama"
    dataset: "The Pile"
    dataset_version: "v1"
    dataset_size: "300B"
    tokenizer_name: "EleutherAI/gpt-neox-20b"

  - name: "HuggingFaceFW/ablation-model-the-pile" 
    revision: "step-065000-136BT"
    cls: "hf"
    batch_size: 100
    device: "cuda"
    architecture: "llama"
    dataset: "The Pile"
    dataset_version: "v1"
    dataset_size: "300B"
    tokenizer_name: "EleutherAI/gpt-neox-20b"

  - name: "HuggingFaceFW/ablation-model-the-pile" 
    revision: "step-070000-147BT"
    cls: "hf"
    batch_size: 100
    device: "cuda"
    architecture: "llama"
    dataset: "The Pile"
    dataset_version: "v1"
    dataset_size: "300B"
    tokenizer_name: "EleutherAI/gpt-neox-20b"

  - name: "HuggingFaceFW/ablation-model-the-pile" 
    revision: "step-075000-157BT"
    cls: "hf"
    batch_size: 100
    device: "cuda"
    architecture: "llama"
    dataset: "The Pile"
    dataset_version: "v1"
    dataset_size: "300B"
    tokenizer_name: "EleutherAI/gpt-neox-20b"

  - name: "HuggingFaceFW/ablation-model-the-pile" 
    revision: "step-080000-168BT"
    cls: "hf"
    batch_size: 100
    device: "cuda"
    architecture: "llama"
    dataset: "The Pile"
    dataset_version: "v1"
    dataset_size: "300B"
    tokenizer_name: "EleutherAI/gpt-neox-20b"

  - name: "HuggingFaceFW/ablation-model-the-pile" 
    cls: "hf"
    batch_size: 100
    device: "cuda"
    architecture: "llama"
    dataset: "The Pile"
    dataset_version: "v1"
    dataset_size: "300B"
    tokenizer_name: "EleutherAI/gpt-neox-20b"

# further checkpoints are not available