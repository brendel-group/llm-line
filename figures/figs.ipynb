{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import figs\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df.copy()\n",
    "filtered_df = filtered_df[~filtered_df[\"Hugging Face\"]]\n",
    "filtered_df = filtered_df[~filtered_df[\"Overtraining\"]]\n",
    "filtered_df = filtered_df[filtered_df[\"Optimizer\"] != \"Adam\"]\n",
    "filtered_df = filtered_df[filtered_df[\"Context Length\"] != 1024]\n",
    "filtered_df = filtered_df[\n",
    "    filtered_df[\"Intervention\"].isin(\n",
    "        [\n",
    "            None,\n",
    "            (\"Architecture\",),\n",
    "            (\"Pretraining Data\",),\n",
    "            (\"Tokenizer\",),\n",
    "            (\"Size\",),\n",
    "            (\"Optimizer\",),\n",
    "            (\"Context Length\",),\n",
    "        ]\n",
    "    )\n",
    "]\n",
    "\n",
    "figs.plot_intervention(\n",
    "    filtered_df,\n",
    "    \"FineWeb-Edu Validation Loss\",\n",
    "    {\n",
    "        \"HellaSwag Test Loss\": [\n",
    "            \"HellaSwag Loss\",\n",
    "        ],\n",
    "    },\n",
    "    \"Intervention\",\n",
    "    plot_group=\"first\",\n",
    "    group_order=[\n",
    "        \"nan\",\n",
    "        \"Pretraining Data\",\n",
    "        \"Tokenizer\",\n",
    "        \"Architecture\",\n",
    "        \"Size\",\n",
    "        \"Context Length\",\n",
    "        \"Optimizer\",\n",
    "    ],\n",
    "    group_names=[\n",
    "        \"None\",\n",
    "        \"Pretraining Data\",\n",
    "        \"Tokenizer\",\n",
    "        \"Architecture\",\n",
    "        \"Size\",\n",
    "        \"Context Length\",\n",
    "        \"Optim. Settings\",\n",
    "    ],\n",
    "    z_order=[3, 3, 3, 2, 2, 2, 3],\n",
    "    x_range=(0, 4.5),\n",
    "    # entropy_df=entropy_df,\n",
    "    fit_curve=[\n",
    "        \"None\",\n",
    "        \"Pretraining Data\",\n",
    "        \"Tokenizer\",\n",
    "        # \"Architecture\",\n",
    "    ],\n",
    "    subsample=3,\n",
    "    title=\"Loss-to-Loss Scaling\",\n",
    "    verbose=True,\n",
    "    legend_kwargs={\n",
    "        \"loc\": \"upper center\",\n",
    "        \"bbox_to_anchor\": (0.5, -0.2),\n",
    "        \"ncol\": 2,\n",
    "    },\n",
    "    save_path=\"fig-overview\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schematic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figs.plot_schematic(save_path=\"fig-schematic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Scaling Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply filters\n",
    "filtered_df = df.copy()\n",
    "filtered_df = filtered_df[~filtered_df[\"Hugging Face\"]]\n",
    "filtered_df = filtered_df[~filtered_df[\"Overtraining\"]]\n",
    "filtered_df = filtered_df[\n",
    "    filtered_df[\"Intervention\"].isin(\n",
    "        [\n",
    "            None,\n",
    "            (\"Pretraining Data\",),\n",
    "            (\"Architecture\",),\n",
    "            (\"Architecture\", \"Pretraining Data\"),\n",
    "        ]\n",
    "    )\n",
    "]\n",
    "# filtered_df = filtered_df[filtered_df[\"FineWeb-Edu Validation Loss\"] < 5]\n",
    "# filtered_df = filtered_df[filtered_df[\"Architecture\"] == \"Mamba\"]\n",
    "# filtered_df = filtered_df[filtered_df[\"Pretraining Data\"] == \"C4\"]\n",
    "\n",
    "archs = filtered_df[\"Architecture\"].unique()\n",
    "pretrains = filtered_df[\"Pretraining Data\"].unique()\n",
    "\n",
    "\n",
    "for arch, pretrain in itertools.product(archs, pretrains):\n",
    "    _df = filtered_df.copy()\n",
    "    _df = _df[_df[\"Architecture\"] == arch]\n",
    "    _df = _df[_df[\"Pretraining Data\"] == pretrain]\n",
    "    _df = _df[_df[f\"{pretrain} Validation Loss\"] < 4.5]\n",
    "    models = _df[\"Name\"].unique()\n",
    "    if len(models) == 0:\n",
    "        continue\n",
    "\n",
    "    print(models)\n",
    "\n",
    "    figs.plot_l2l(\n",
    "        _df,\n",
    "        f\"{pretrain} Validation Loss\",\n",
    "        {\n",
    "            \"Validation Loss\": [\n",
    "                \"The Pile UC Validation Loss\",\n",
    "                \"RefineWeb Validation Loss\",\n",
    "                \"Slimpajama Validation Loss\",\n",
    "                \"C4 Validation Loss\",\n",
    "            ],\n",
    "            \"Test Loss\": [\n",
    "                \"ARC-Challenge Loss\",\n",
    "                \"ARC-Easy Loss\",\n",
    "                \"OpenBookQA Loss\",\n",
    "                \"PIQA Loss\",\n",
    "                \"COPA Loss\",\n",
    "                \"Winogrande Loss\",\n",
    "                \"HellaSwag Loss\",\n",
    "                # \"CommonSenseQA Loss\",\n",
    "                # \"Social IQa Loss\",\n",
    "                # \"MMLU Loss\",\n",
    "            ],\n",
    "        },\n",
    "        fit_curves=True,\n",
    "        # entropy_df=entropy_df,\n",
    "        titles=[\"Train-to-Train\", \"Train-to-Test\"],\n",
    "        # titles=[\"Train-to-Test\"],\n",
    "        legend_kwargs=[\n",
    "            {\n",
    "                \"loc\": \"upper center\",\n",
    "                \"bbox_to_anchor\": (0.5, -0.2),\n",
    "                \"ncol\": 1,\n",
    "                \"title\": \"Validation Set\",\n",
    "            },\n",
    "            {\n",
    "                \"loc\": \"upper center\",\n",
    "                \"bbox_to_anchor\": (0.5, -0.2),\n",
    "                \"ncol\": 1,\n",
    "                \"title\": \"Test Set\",\n",
    "            },\n",
    "        ],\n",
    "        e_min=0.1,\n",
    "        save_path=f\"fig-l2l-all_{arch.lower().replace(' ', '-')}_{pretrain.lower().replace(' ', '-')}\",\n",
    "        # verbose=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intervening on Pretraining Data, Tokenizer, and Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df.copy()\n",
    "filtered_df = filtered_df[~filtered_df[\"Overtraining\"]]\n",
    "\n",
    "\n",
    "def tuple_contains_any_combination(row, vals):\n",
    "    if row is None:\n",
    "        if None in vals:\n",
    "            return True\n",
    "        return False\n",
    "    return all(r in vals for r in row)\n",
    "\n",
    "\n",
    "filtered_df = filtered_df[\n",
    "    filtered_df[\"Intervention\"].apply(\n",
    "        tuple_contains_any_combination,\n",
    "        vals=[None, \"Pretraining Data\", \"Tokenizer\", \"Architecture\"],\n",
    "    )\n",
    "]\n",
    "\n",
    "figs.plot_intervention_matched_dims(\n",
    "    filtered_df,\n",
    "    \"Pretraining Data\",\n",
    "    [\"Architecture\", \"Tokenizer\"],\n",
    "    [\n",
    "        [\"Llama\", \"tiktoken\"],\n",
    "        [\"Llama\", \"gpt2\"],\n",
    "        [\"Llama\", \"gpt2-HF\"],  # could kick this\n",
    "        [\"Mamba\", \"tiktoken\"],\n",
    "        [\"Mamba\", \"gpt2\"],\n",
    "        [\"GPT\", \"gpt-neox\"],\n",
    "    ],\n",
    "    \"FineWeb-Edu Validation Loss\",\n",
    "    {\n",
    "        \"C4 Validation Loss\": [\n",
    "            # \"The Pile UC Validation Loss\",\n",
    "            # \"FineWeb-Edu Validation Loss\",\n",
    "            # \"RefineWeb Validation Loss\",\n",
    "            # \"Slimpajama Validation Loss\",\n",
    "            \"C4 Validation Loss\",\n",
    "        ],\n",
    "        \"ARC-Easy Test Loss\": [\n",
    "            # \"ARC-Challenge Loss\",\n",
    "            \"ARC-Easy Loss\",\n",
    "            # \"OpenBookQA Loss\",\n",
    "            # \"PIQA Loss\",\n",
    "            # \"COPA Loss\",\n",
    "            # \"Winogrande Loss\",\n",
    "            # \"HellaSwag Loss\",\n",
    "        ],\n",
    "    },\n",
    "    [\"Train-to-Train\", \"Train-to-Test\"],\n",
    "    title_x_pos=0.6,\n",
    "    fit_curve=True,\n",
    "    x_lim_upper=4.5,\n",
    "    x_name=\"FW-Edu Val. Loss\",\n",
    "    # verbose=True,\n",
    "    legend_kwargs={\n",
    "        \"loc\": \"upper center\",\n",
    "        \"bbox_to_anchor\": (0.5, 0),\n",
    "        \"ncol\": 5,\n",
    "    },\n",
    "    # save_path=\"fig-intervention-matched_pretraining_specific2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df.copy()\n",
    "filtered_df = filtered_df[~filtered_df[\"Overtraining\"]]\n",
    "\n",
    "\n",
    "def tuple_contains_any_combination(row, vals):\n",
    "    if row is None:\n",
    "        if None in vals:\n",
    "            return True\n",
    "        return False\n",
    "    return all(r in vals for r in row)\n",
    "\n",
    "\n",
    "filtered_df = filtered_df[\n",
    "    filtered_df[\"Intervention\"].apply(\n",
    "        tuple_contains_any_combination,\n",
    "        vals=[None, \"Pretraining Data\", \"Tokenizer\", \"Architecture\"],\n",
    "    )\n",
    "]\n",
    "\n",
    "figs.plot_intervention_matched_dims(\n",
    "    filtered_df,\n",
    "    \"Tokenizer\",\n",
    "    [\"Architecture\", \"Pretraining Data\"],\n",
    "    [\n",
    "        [\"Llama\", \"C4\"],\n",
    "        [\"Llama\", \"FineWeb-Edu\"],\n",
    "        [\"Llama\", \"The Pile UC\"],\n",
    "        [\"Mamba\", \"C4\"],\n",
    "        # [\"Mamba\", \"FineWeb-EDU\"],\n",
    "        [\"Mamba\", \"The Pile UC\"],\n",
    "        [\"GPT\", \"The Pile\"],\n",
    "    ],\n",
    "    # \"C4 Validation Loss\",\n",
    "    \"FineWeb-Edu Validation Loss\",\n",
    "    {\n",
    "        # \"Average Validation Loss\": [\n",
    "        # \"The Pile UC Val. Loss\": [\n",
    "        \"C4 Validation Loss\": [\n",
    "            # \"The Pile UC Validation Loss\",\n",
    "            # \"FineWeb-Edu Validation Loss\",\n",
    "            # \"RefineWeb Validation Loss\",\n",
    "            # \"Slimpajama Validation Loss\",\n",
    "            \"C4 Validation Loss\",\n",
    "        ],\n",
    "        # \"Average Test Loss\": [\n",
    "        # \"HellaSwag Test Loss\": [\n",
    "        \"ARC-Easy Test Loss\": [\n",
    "            # \"ARC-Challenge Loss\",\n",
    "            \"ARC-Easy Loss\",\n",
    "            # \"OpenBookQA Loss\",\n",
    "            # \"PIQA Loss\",\n",
    "            # \"COPA Loss\",\n",
    "            # \"Winogrande Loss\",\n",
    "            # \"HellaSwag Loss\",\n",
    "        ],\n",
    "    },\n",
    "    [\"Train-to-Train\", \"Train-to-Test\"],\n",
    "    title_x_pos=0.6,\n",
    "    fit_curve=True,\n",
    "    x_lim_upper=4.5,\n",
    "    # verbose=True,\n",
    "    x_name=\"FW-Edu Val. Loss\",\n",
    "    shorten_col_titles=True,\n",
    "    legend_kwargs={\n",
    "        \"loc\": \"upper center\",\n",
    "        \"bbox_to_anchor\": (0.5, 0),\n",
    "        \"ncol\": 4,\n",
    "    },\n",
    "    save_path=\"fig-intervention-matched_tokenizer_specific2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df.copy()\n",
    "filtered_df = filtered_df[~filtered_df[\"Overtraining\"]]\n",
    "filtered_df = filtered_df[\n",
    "    filtered_df[\"Intervention\"].apply(\n",
    "        figs.tuple_contains_any_combination,\n",
    "        vals=[None, \"Pretraining Data\", \"Tokenizer\", \"Architecture\"],\n",
    "    )\n",
    "]\n",
    "\n",
    "figs.plot_intervention_matched_dims(\n",
    "    filtered_df,\n",
    "    \"Architecture\",\n",
    "    [\"Pretraining Data\", \"Tokenizer\"],\n",
    "    [\n",
    "        [\"C4\", \"gpt2\"],\n",
    "        [\"C4\", \"tiktoken\"],\n",
    "        [\"FineWeb-Edu\", \"gpt2\"],\n",
    "        [\"FineWeb-Edu\", \"tiktoken\"],\n",
    "        [\"The Pile UC\", \"tiktoken\"],\n",
    "        [\"The Pile\", \"gpt-neox\"],\n",
    "    ],\n",
    "    # \"C4 Validation Loss\",\n",
    "    \"FineWeb-Edu Validation Loss\",\n",
    "    {\n",
    "        # \"Average Validation Loss\": [\n",
    "        # \"The Pile UC Val. Loss\": [\n",
    "        \"C4 Validation Loss\": [\n",
    "            # \"The Pile UC Validation Loss\",\n",
    "            # \"FineWeb-Edu Validation Loss\",\n",
    "            # \"RefineWeb Validation Loss\",\n",
    "            # \"Slimpajama Validation Loss\",\n",
    "            \"C4 Validation Loss\",\n",
    "        ],\n",
    "        # \"Average Test Loss\": [\n",
    "        # \"HellaSwag Test Loss\": [\n",
    "        \"ARC-Easy Test Loss\": [\n",
    "            # \"ARC-Challenge Loss\",\n",
    "            \"ARC-Easy Loss\",\n",
    "            # \"OpenBookQA Loss\",\n",
    "            # \"PIQA Loss\",\n",
    "            # \"COPA Loss\",\n",
    "            # \"Winogrande Loss\",\n",
    "            # \"HellaSwag Loss\",\n",
    "        ],\n",
    "    },\n",
    "    [\"Train-to-Train\", \"Train-to-Test\"],\n",
    "    shorten_col_titles=True,\n",
    "    title_x_pos=0.6,\n",
    "    fit_curve=True,\n",
    "    x_lim_upper=4.5,\n",
    "    # verbose=True,\n",
    "    x_name=\"FW-Edu Val. Loss\",\n",
    "    legend_kwargs={\n",
    "        \"loc\": \"upper center\",\n",
    "        \"bbox_to_anchor\": (0.5, 0),\n",
    "        \"ncol\": 4,\n",
    "    },\n",
    "    save_path=\"fig-intervention-matched_architecture_specific2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df.copy()\n",
    "filtered_df = filtered_df[~filtered_df[\"Hugging Face\"]]\n",
    "filtered_df = filtered_df[~filtered_df[\"Overtraining\"]]\n",
    "filtered_df = filtered_df[\n",
    "    filtered_df[\"Intervention\"].apply(\n",
    "        figs.tuple_contains_any_combination,\n",
    "        vals=[None, \"Size\", \"Pretraining Data\", \"Architecture\"],\n",
    "    )\n",
    "]\n",
    "\n",
    "figs.plot_intervention_size(\n",
    "    filtered_df,\n",
    "    \"FineWeb-Edu Validation Loss\",\n",
    "    {\n",
    "        # \"Average Validation Loss\": [\n",
    "        #     \"The Pile UC Validation Loss\",\n",
    "        #     # \"FineWeb-Edu Validation Loss\",\n",
    "        #     \"RefineWeb Validation Loss\",\n",
    "        #     \"Slimpajama Validation Loss\",\n",
    "        #     \"C4 Validation Loss\",\n",
    "        # ],\n",
    "        \"AverageTest Loss\": [\n",
    "            \"ARC-Challenge Loss\",\n",
    "            \"ARC-Easy Loss\",\n",
    "            \"OpenBookQA Loss\",\n",
    "            \"PIQA Loss\",\n",
    "            \"COPA Loss\",\n",
    "            \"Winogrande Loss\",\n",
    "            \"HellaSwag Loss\",\n",
    "        ],\n",
    "    },\n",
    "    markersize=5,\n",
    "    tokenizer=\"tiktoken\",\n",
    "    # arch=\"Mamba\",\n",
    "    # pretrain=[\"The Pile UC\", \"FineWeb-EDU\"],\n",
    "    save_path=\"fig-intervention-size_fw_test\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df.copy()\n",
    "filtered_df = filtered_df[~filtered_df[\"Hugging Face\"]]\n",
    "filtered_df = filtered_df[~filtered_df[\"Overtraining\"]]\n",
    "filtered_df = filtered_df[\n",
    "    filtered_df[\"Intervention\"].apply(\n",
    "        figs.tuple_contains_any_combination,\n",
    "        vals=[None, \"Optimizer\", \"Pretraining Data\", \"Architecture\"],\n",
    "    )\n",
    "]\n",
    "# Filter models by name where the minimum loss on C4 Validation is below 7\n",
    "# filtered_df = filtered_df.groupby(\"Name\").filter(lambda x: x[\"C4 Validation Loss\"].min() < 7)\n",
    "filtered_df = filtered_df[filtered_df[\"C4 Validation Loss\"] < 7]\n",
    "\n",
    "\n",
    "print(len(filtered_df[\"Name\"].unique()))\n",
    "\n",
    "figs.plot_intervention_optim(\n",
    "    filtered_df,\n",
    "    \"FineWeb-Edu Validation Loss\",\n",
    "    {\n",
    "        \"Average Validation Loss\": [\n",
    "            \"The Pile UC Validation Loss\",\n",
    "            # \"FineWeb-Edu Validation Loss\",\n",
    "            \"RefineWeb Validation Loss\",\n",
    "            \"Slimpajama Validation Loss\",\n",
    "            \"C4 Validation Loss\",\n",
    "        ],\n",
    "        # \"AverageTest Loss\": [\n",
    "        #     \"ARC-Challenge Loss\",\n",
    "        #     \"ARC-Easy Loss\",\n",
    "        #     \"OpenBookQA Loss\",\n",
    "        #     \"PIQA Loss\",\n",
    "        #     \"COPA Loss\",\n",
    "        #     \"Winogrande Loss\",\n",
    "        #     \"HellaSwag Loss\",\n",
    "        # ],\n",
    "    },\n",
    "    tokenizer=\"tiktoken\",\n",
    "    # arch=\"Mamba\",\n",
    "    pretrain=\"FineWeb-Edu\",\n",
    "    # pretrain=[\"The Pile UC\", \"FineWeb-EDU\"],\n",
    "    save_path=\"fig-intervention-optim_fw\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context Length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df.copy()\n",
    "filtered_df = filtered_df[~filtered_df[\"Hugging Face\"]]\n",
    "filtered_df = filtered_df[~filtered_df[\"Overtraining\"]]\n",
    "filtered_df = filtered_df[\n",
    "    filtered_df[\"Intervention\"].apply(\n",
    "        figs.tuple_contains_any_combination,\n",
    "        vals=[None, \"Context Length\", \"Pretraining Data\", \"Architecture\"],\n",
    "    )\n",
    "]\n",
    "\n",
    "figs.plot_intervention_ctx(\n",
    "    filtered_df,\n",
    "    \"FineWeb-Edu Validation Loss\",\n",
    "    {\n",
    "        \"Average Validation Loss\": [\n",
    "            \"The Pile UC Validation Loss\",\n",
    "            # \"FineWeb-Edu Validation Loss\",\n",
    "            \"RefineWeb Validation Loss\",\n",
    "            \"Slimpajama Validation Loss\",\n",
    "            \"C4 Validation Loss\",\n",
    "        ],\n",
    "        # \"AverageTest Loss\": [\n",
    "        #     \"ARC-Challenge Loss\",\n",
    "        #     \"ARC-Easy Loss\",\n",
    "        #     \"OpenBookQA Loss\",\n",
    "        #     \"PIQA Loss\",\n",
    "        #     \"COPA Loss\",\n",
    "        #     \"Winogrande Loss\",\n",
    "        #     \"HellaSwag Loss\",\n",
    "        # ],\n",
    "    },\n",
    "    tokenizer=\"tiktoken\",\n",
    "    markersize=20,\n",
    "    # arch=\"Mamba\",\n",
    "    # pretrain=[\"The Pile UC\", \"FineWeb-EDU\"],\n",
    "    save_path=\"fig-intervention-ctx_fw\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-line",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
