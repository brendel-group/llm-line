{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import figs\n",
    "import utils\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to compare\n",
    "columns_to_compare = [\n",
    "    \"c4_val_loss\",\n",
    "    \"pile_uc_val_loss\",\n",
    "    \"fineweb_edu_100bt_val_loss\",\n",
    "    \"refineweb_val_loss\",\n",
    "    \"slimpajama_val_loss\",\n",
    "    \"arc_challenge/loss\",\n",
    "    \"arc_easy/loss\",\n",
    "    \"hellaswag/loss\",\n",
    "    \"piqa/loss\",\n",
    "    \"openbookqa/loss\",\n",
    "    \"winogrande/loss\",\n",
    "]\n",
    "\n",
    "# Apply filters\n",
    "filtered_df = filtered_df.copy()\n",
    "# filtered_df, valid_models = utils.filter_min_tokens(filtered_df, min_tokens=7.5e9)\n",
    "# filtered_df = utils.filter_within_chinchilla(filtered_df)\n",
    "# filtered_df = filtered_df[filtered_df[\"size\"].between(400e6, 430e6)]\n",
    "# filtered_df = filtered_df[filtered_df[\"n_layers\"].between(23, 26)]\n",
    "filtered_df = filtered_df[filtered_df[\"tokenizer\"] == \"tiktoken\"]\n",
    "# filtered_df = filtered_df[filtered_df[\"position_embedding\"] == \"rope\"]\n",
    "# filtered_df = filtered_df[filtered_df[\"norm_type\"] == \"rms_norm\"]\n",
    "\n",
    "# Create the grid plot\n",
    "utils.create_grid_comparison(\n",
    "    filtered_df,\n",
    "    columns_to_compare,\n",
    "    # transform=\"log\",\n",
    "    save_path=\"grid-all_tiktoken.png\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to compare\n",
    "columns_to_compare = [\n",
    "    \"c4_val_loss\",\n",
    "    \"pile_uc_val_loss\",\n",
    "    \"fineweb_edu_100bt_val_loss\",\n",
    "    \"refineweb_val_loss\",\n",
    "    \"slimpajama_val_loss\",\n",
    "]\n",
    "\n",
    "# Apply filters\n",
    "filtered_df = df.copy()\n",
    "# filtered_df, valid_models = utils.filter_min_tokens(filtered_df, min_tokens=7.5e9)\n",
    "# filtered_df = utils.filter_within_chinchilla(filtered_df)\n",
    "# filtered_df = filtered_df[filtered_df[\"size\"].between(400e6, 430e6)]\n",
    "# filtered_df = filtered_df[filtered_df[\"n_layers\"].between(23, 26)]\n",
    "\n",
    "# Create the grid plot\n",
    "utils.create_grid_comparison(\n",
    "    filtered_df,\n",
    "    columns_to_compare,\n",
    "    # transform=\"log\",\n",
    "    save_path=\"grid-val.png\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to compare\n",
    "columns_to_compare = [\n",
    "    \"arc_challenge/loss\",\n",
    "    \"arc_easy/loss\",\n",
    "    \"hellaswag/loss\",\n",
    "    \"piqa/loss\",\n",
    "    \"openbookqa/loss\",\n",
    "    \"winogrande/loss\",\n",
    "]\n",
    "\n",
    "# Apply filters\n",
    "filtered_df = df.copy()\n",
    "filtered_df, valid_models = utils.filter_min_tokens(filtered_df, min_tokens=7.5e9)\n",
    "filtered_df = utils.filter_within_chinchilla(filtered_df)\n",
    "# filtered_df = filtered_df[filtered_df[\"size\"].between(400e6, 430e6)]\n",
    "# filtered_df = filtered_df[filtered_df[\"n_layers\"].between(23, 26)]\n",
    "\n",
    "# Create the grid plot\n",
    "utils.create_grid_comparison(\n",
    "    filtered_df,\n",
    "    columns_to_compare,\n",
    "    # transform=\"log\",\n",
    "    # save_path=\"grid-task_min-tokens7.5e9_chinchilla.png\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss-to-Loss plots\n",
    "Joint plots as in the \"Scaling Laws for Loss-to-Loss\" paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df.copy()\n",
    "filtered_df = filtered_df[filtered_df[\"tokenizer\"] == \"tiktoken\"]\n",
    "# filtered_df = filtered_df[filtered_df[\"position_embedding\"].isin([\"rope\", np.nan])]\n",
    "# filtered_df = filtered_df[filtered_df[\"norm_type\"] == \"rms_norm\"]\n",
    "\n",
    "utils.plot_joint_train2train_v2(\n",
    "    filtered_df,\n",
    "    save_path=\"train2train2_tiktoken.png\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df.copy()\n",
    "filtered_df = filtered_df[filtered_df[\"tokenizer\"] == \"tiktoken\"]\n",
    "filtered_df = filtered_df[filtered_df[\"context_length\"] == 2048]\n",
    "\n",
    "figs.plot_fit_vs_accuracy(filtered_df, \"llama\", \"c4\", \"fig_fit_vs_accuracy_llama_c4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intervene on Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df.copy()\n",
    "\n",
    "# TODO: fix this once we have evals with the same tokenizer\n",
    "\n",
    "# Fix pretraining data\n",
    "# GPT models were trained on the Pile or the Pile deduped depending on the name\n",
    "filtered_df.loc[\n",
    "    (filtered_df[\"arch\"] == \"GPT\") & (filtered_df[\"name\"].str.contains(\"deduped\")),\n",
    "    \"pretraining_data\",\n",
    "] = \"The Pile deduped\"\n",
    "filtered_df.loc[\n",
    "    (filtered_df[\"arch\"] == \"GPT\") & ~(filtered_df[\"name\"].str.contains(\"deduped\")),\n",
    "    \"pretraining_data\",\n",
    "] = \"The Pile\"\n",
    "# LLama models were trained on the Pile deduped\n",
    "filtered_df.loc[\n",
    "    (filtered_df[\"arch\"] == \"llama\") & (filtered_df[\"pretraining_data\"] == \"The Pile\"),\n",
    "    \"pretraining_data\",\n",
    "] = \"The Pile deduped\"\n",
    "\n",
    "# Make difference between GPT models apparent\n",
    "filtered_df.loc[\n",
    "    filtered_df[\"name\"].str.contains(\"neo\"),\n",
    "    \"arch\",\n",
    "] = \"GPT neo\"\n",
    "filtered_df.loc[\n",
    "    filtered_df[\"name\"].str.contains(\"neox\"),\n",
    "    \"arch\",\n",
    "] = \"GPT neox\"\n",
    "filtered_df.loc[\n",
    "    filtered_df[\"name\"].str.contains(\"pythia\"),\n",
    "    \"arch\",\n",
    "] = \"GPT pythia\"\n",
    "filtered_df.loc[\n",
    "    filtered_df[\"name\"].str.contains(\"gpt-j\"),\n",
    "    \"arch\",\n",
    "] = \"GPT j\"\n",
    "\n",
    "# For now, only keep Mamba, Mamba2, GPT-pythia, PGT-neox, all of which use the same\n",
    "filtered_df = filtered_df[\n",
    "    filtered_df[\"arch\"].isin([\"Mamba\", \"Mamba2\", \"GPT pythia\", \"GPT neox\"])\n",
    "]\n",
    "\n",
    "figs.plot_intervention_arch(\n",
    "    filtered_df, avg_only=False, save_path=\"fig_intervention_arch\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jointly Intervene on Architecture, Tokenizer, Pretraining Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = utils.load_and_prepare_latest(\"df_combined\")\n",
    "\n",
    "# Amend data\n",
    "# Make architecture explicit\n",
    "df.loc[df[\"Architecture\"] == \"mamba\", \"Architecture\"] = \"Mamba\"\n",
    "df.loc[df[\"Architecture\"] == \"llama\", \"Architecture\"] = \"LLaMA\"\n",
    "\n",
    "# Make tokenizer explicit\n",
    "df.loc[\n",
    "    df[\"Hugging Face\"] & (df[\"Name\"].str.contains(\"neox|pythia|mamba|mamba2\")),\n",
    "    \"Tokenizer\",\n",
    "] = \"gpt-neox\"\n",
    "df.loc[\n",
    "    df[\"Hugging Face\"] & (df[\"Name\"].str.contains(\"gpt-j|neo-|ablation\")), \"Tokenizer\"\n",
    "] = \"gpt2-HF\"\n",
    "df.loc[df[\"Hugging Face\"] & (df[\"Name\"].str.contains(\"Qwen\")), \"Tokenizer\"] = \"qwen\"\n",
    "\n",
    "# Make pretraining data explicit\n",
    "df.loc[df[\"Pretraining Data\"] == \"FineWeb-EDU\", \"Pretraining Data\"] = \"FineWeb-EDU\"\n",
    "df.loc[df[\"Pretraining Data\"] == \"fineweb_edu_100bt\", \"Pretraining Data\"] = (\n",
    "    \"FineWeb-EDU\"\n",
    ")\n",
    "df.loc[df[\"Pretraining Data\"] == \"c4\", \"Pretraining Data\"] = \"C4\"\n",
    "df.loc[df[\"Pretraining Data\"] == \"pile_uc\", \"Pretraining Data\"] = \"The Pile UC\"\n",
    "# GPT models were trained on the Pile or the Pile deduped depending on the name\n",
    "df.loc[\n",
    "    (df[\"Architecture\"] == \"GPT\") & (df[\"Name\"].str.contains(\"deduped\")),\n",
    "    \"Pretraining Data\",\n",
    "] = \"The Pile Deduped\"\n",
    "df.loc[\n",
    "    (df[\"Architecture\"] == \"GPT\") & ~(df[\"Name\"].str.contains(\"deduped\")),\n",
    "    \"Pretraining Data\",\n",
    "] = \"The Pile\"\n",
    "# LLama models were trained on the Pile deduped\n",
    "df.loc[\n",
    "    (df[\"Architecture\"] == \"LLaMA\")\n",
    "    & (df[\"Pretraining Data\"] == \"The Pile\")\n",
    "    & df[\"Hugging Face\"],\n",
    "    \"Pretraining Data\",\n",
    "] = \"The Pile Deduped\"\n",
    "\n",
    "# Make base models explicit\n",
    "df[\"Size Intervention\"] = df[\"Name\"].str.contains(\"_d_|_l_\")\n",
    "df[\"Context Length Intervention\"] = df[\"Name\"].str.contains(\"_ctx_\")\n",
    "df[\"Optimizer Intervention\"] = df[\"Name\"].str.contains(\"_lr_|_wd_|adam|cosine\")\n",
    "df[\"Is Base\"] = ~(\n",
    "    df[\"Size Intervention\"]\n",
    "    | df[\"Context Length Intervention\"]\n",
    "    | df[\"Optimizer Intervention\"]\n",
    ")\n",
    "\n",
    "# Rename columns\n",
    "legible_columns = {\n",
    "    \"c4\": \"C4\",\n",
    "    \"pile_uc\": \"The Pile UC\",\n",
    "    \"fineweb_edu_100bt\": \"FineWeb-EDU\",\n",
    "    \"refineweb\": \"RefineWeb\",\n",
    "    \"slimpajama\": \"Slimpajama\",\n",
    "    \"arc_challenge\": \"ARC-Challenge\",\n",
    "    \"arc_easy\": \"ARC-Easy\",\n",
    "    \"openbookqa\": \"OpenBookQA\",\n",
    "    \"piqa\": \"PIQA\",\n",
    "    \"copa\": \"COPA\",\n",
    "    \"winogrande\": \"Winogrande\",\n",
    "    \"hellaswag\": \"HellaSwag\",\n",
    "    \"mmlu\": \"MMLU\",\n",
    "    \"social_iqa\": \"Social IQa\",\n",
    "    \"commonsenseqa\": \"CommonSenseQA\",\n",
    "}\n",
    "for col in df.columns:\n",
    "    for old, new in legible_columns.items():\n",
    "        if old in col:\n",
    "            df.rename(columns={col: col.replace(old, new)}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df.copy()\n",
    "filtered_df = filtered_df[filtered_df[\"Is Base\"]]\n",
    "\n",
    "figs.plot_intervention_arch_tokenizer_pretraining(\n",
    "    filtered_df,\n",
    "    # save_path=\"debug_arch_tokenizer_pretraining_val2test\",\n",
    "    xdata=\"C4 Validation Loss\",\n",
    "    ydata=[\n",
    "        \"ARC-Challenge Loss\",\n",
    "        \"ARC-Easy Loss\",\n",
    "        \"OpenBookQA Loss\",\n",
    "        \"PIQA Loss\",\n",
    "        \"COPA Loss\",\n",
    "        \"Winogrande Loss\",\n",
    "        \"HellaSwag Loss\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intervene on Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = utils.load_latest(\"df_train_scratch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add optimizer columns\n",
    "# lr 3e-3, 3e-4\n",
    "df[\"lr\"]\n",
    "# wd 0.1 0.033\n",
    "# optimizer adam, adamw\n",
    "# scheduler cosine, wsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"name\"].unique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-line",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
