{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deprecated, please use `create_df.py` and `plot_grids.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "import utils\n",
    "from mmlu_utils import compute_avg_losses, compute_avg_task_losses_and_accuracies\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get data and basic analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get df\n",
    "# dir_logs = '/fast/pmayilvahanan/lm_logs/lingua_old/'\n",
    "dir_logs = \"/fast/pmayilvahanan/lm_logs/lingua/\"\n",
    "df = utils.gather_experiment_data(dir_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save df\n",
    "df.to_csv(\"/lustre/fast/fast/pmayilvahanan/llm_line/code/llm_line/notebooks/df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = copy.deepcopy(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop nan values\n",
    "df.dropna(subset=[\"train_loss\", \"test_loss\"], how=\"any\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"tokenizer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# at the moment, only analyze data for jobs that are (almost) completed\n",
    "\n",
    "# After loading df but before other processing\n",
    "min_tokens = 7.5e9  # 8.4B tokens minimum, adjust as needed\n",
    "\n",
    "# Get max tokens for each model name\n",
    "max_tokens_per_model = df.groupby(\"name\")[\"tokens\"].max()\n",
    "\n",
    "# Keep only models that reached the minimum token count\n",
    "valid_models = max_tokens_per_model[max_tokens_per_model >= min_tokens].index\n",
    "df = df[df[\"name\"].isin(valid_models)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add_feature, within_chinchilla (if tokens < size*20)\n",
    "df[\"within_chinchilla\"] = df[\"tokens\"] < df[\"size\"] * 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mmlu avg (computing average of all mmlu task losses)\n",
    "df[\n",
    "    [\n",
    "        \"mmlu/loss\",\n",
    "        \"mmlu_stem/loss\",\n",
    "        \"mmlu_other/loss\",\n",
    "        \"mmlu_humanities/loss\",\n",
    "        \"mmlu_social_sciences/loss\",\n",
    "    ]\n",
    "] = df.apply(extract_mmlu_losses, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute average of losses and accuracies of given tasks\n",
    "task_columns = [\n",
    "    \"hellaswag\",\n",
    "    \"piqa\",\n",
    "    \"arc_easy\",\n",
    "    \"arc_challenge\",\n",
    "    \"commonsense_qa\",\n",
    "    \"openbookqa\",\n",
    "    \"winogrande\",\n",
    "    \"social_iqa\",\n",
    "    \"mmlu\",\n",
    "]\n",
    "task_columns_loss = [f\"{t}/loss\" for t in task_columns]\n",
    "task_columns_acc = [f\"{t}/acc\" for t in task_columns]\n",
    "df[\"avg/loss\"] = df[task_columns_loss].mean(axis=1)\n",
    "df[\"avg/acc\"] = df[task_columns_acc].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or load from csv\n",
    "df = pd.read_csv(\"df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop nan values\n",
    "df.dropna(subset=[\"train_loss\", \"test_loss\"], how=\"any\", inplace=True)\n",
    "\n",
    "# compute avg losses and accuracies\n",
    "tasks = [\n",
    "    \"hellaswag\",\n",
    "    \"piqa\",\n",
    "    \"arc_easy\",\n",
    "    \"arc_challenge\",\n",
    "    \"commonsense_qa\",\n",
    "    \"openbookqa\",\n",
    "    \"winogrande\",\n",
    "    \"social_iqa\",\n",
    "    \"mmlu\",\n",
    "]\n",
    "df = compute_avg_losses(df)\n",
    "df = compute_avg_task_losses_and_accuracies(df, tasks=tasks)\n",
    "tasks.remove(\"hellaswag\")\n",
    "df = compute_avg_task_losses_and_accuracies(\n",
    "    df, tasks=tasks, col_name=\"avg_minus_hellaswag\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does overfitting happen?\n",
    "plt.plot(df[df[\"name\"] == \"mamba_420M_fw_edu_8.4BT\"][\"mmlu/loss\"])\n",
    "plt.xlabel(\"steps\")\n",
    "plt.ylabel(\"val loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"pretraining_data\"] == \"pile_uc\"][\"commonsense_qa/loss\"].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"pretraining_data\"] == \"pile_uc\"][\"commonsense_qa/acc\"].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_columns = [\"name\", \"arch\", \"size\", \"dim\", \"n_layers\", \"pretraining_data\"]\n",
    "task_columns = [\n",
    "    \"hellaswag\",\n",
    "    \"piqa\",\n",
    "    \"arc_easy\",\n",
    "    \"arc_challenge\",\n",
    "    \"openbookqa\",\n",
    "    \"commonsense_qa\",\n",
    "    \"winogrande\",\n",
    "    \"social_iqa\",\n",
    "    \"mmlu\",\n",
    "    \"avg\",\n",
    "]\n",
    "val_columns = [\n",
    "    \"slimpajama_val_loss\",\n",
    "    \"c4_val_loss\",\n",
    "    \"pile_uc_val_loss\",\n",
    "    \"fineweb_edu_100bt_val_loss\",\n",
    "    \"refineweb_val_loss\",\n",
    "]\n",
    "\n",
    "# Correlation targets: val_columns + each \"task/acc\" + each \"task/loss\"\n",
    "corr_targets = (\n",
    "    val_columns\n",
    "    + [f\"{t}/acc\" for t in task_columns]\n",
    "    + [f\"{t}/loss\" for t in task_columns]\n",
    ")\n",
    "\n",
    "\n",
    "# Helper to compute correlation if there are at least two non-null data points\n",
    "def safe_corrcoef(x, y):\n",
    "    if x.notna().sum() < 2 or y.notna().sum() < 2:\n",
    "        return np.nan\n",
    "    return np.corrcoef(x, y)[0, 1]\n",
    "\n",
    "\n",
    "corr_rows = []\n",
    "\n",
    "for model_id, group in df.groupby(basic_columns, dropna=False):\n",
    "    # model_id is a tuple of values for (name, arch, size, dim, n_layers, pretraining_data)\n",
    "    for anchor_loss in [\"test_loss\", \"train_loss\", \"hellaswag/loss\"]:\n",
    "        if anchor_loss not in group.columns:\n",
    "            continue\n",
    "\n",
    "        # Build one row per (model, anchor_loss)\n",
    "        row = dict(zip(basic_columns, model_id))\n",
    "        row[\"anchor\"] = anchor_loss\n",
    "\n",
    "        for target_col in corr_targets:\n",
    "            row[f\"{target_col}\"] = (\n",
    "                safe_corrcoef(group[anchor_loss], group[target_col])\n",
    "                if target_col in group.columns\n",
    "                else np.nan\n",
    "            )\n",
    "\n",
    "        corr_rows.append(row)\n",
    "\n",
    "corr_all = pd.DataFrame(corr_rows)\n",
    "corr_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's just look at 'test_loss' (in-d loss) correlations atm\n",
    "corr = corr_all[corr_all[\"anchor\"] == \"test_loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered\n",
    "task = \"mmlu\"\n",
    "metric = \"loss\"\n",
    "threshold = 0.7\n",
    "filtered_corr = corr[corr[f\"{task}/{metric}\"] < threshold]\n",
    "\n",
    "# report\n",
    "columns_to_report = (\n",
    "    [\"name\", \"pretraining_data\"] + val_columns + [f\"{t}/loss\" for t in task_columns]\n",
    ")\n",
    "print(len(filtered_corr[columns_to_report]))\n",
    "filtered_corr[columns_to_report]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation change\n",
    "\n",
    "\n",
    "def compare_correlations(df, base_name, intervention_name, col1, col2):\n",
    "    \"\"\"\n",
    "    Given a DataFrame, two 'name' values (base and intervention), and two columns,\n",
    "    prints the correlation for each name separately, the combined correlation,\n",
    "    and the percentage change in correlation from base to combined.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    base_df = df[df[\"name\"] == base_name]\n",
    "    int_df = df[df[\"name\"] == intervention_name]\n",
    "    comb_df = df[df[\"name\"].isin([base_name, intervention_name])]\n",
    "\n",
    "    corr_base = base_df[col1].corr(base_df[col2])\n",
    "    corr_int = int_df[col1].corr(int_df[col2])\n",
    "    corr_comb = comb_df[col1].corr(comb_df[col2])\n",
    "\n",
    "    # Compute percentage change from base correlation to combined\n",
    "    if corr_base and not np.isnan(corr_base) and corr_base != 0:\n",
    "        pct_change = 100.0 * (corr_comb - corr_base) / abs(corr_base)\n",
    "    else:\n",
    "        pct_change = None\n",
    "\n",
    "    print(f\"Base ({base_name}) correlation: {corr_base:.3f}\")\n",
    "    print(f\"Intervention ({intervention_name}) correlation: {corr_int:.3f}\")\n",
    "    print(f\"Combined correlation: {corr_comb:.3f}\")\n",
    "    if pct_change is not None:\n",
    "        print(f\"Percentage change from base to combined: {pct_change:.2f}%\")\n",
    "    else:\n",
    "        print(\"Percentage change from base to combined: N/A (invalid base corr).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = df[\"name\"].unique()\n",
    "temp.sort()\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# depth intervention\n",
    "\n",
    "for dataset in [\"fw_edu\", \"c4\", \"pile_uc\"]:\n",
    "    base_name = \"llama_416M_{dataset}_8.4BT\".format(dataset=dataset)\n",
    "    intervention_name = \"mamba_420M_{dataset}_8.4BT\".format(dataset=dataset)\n",
    "    col1 = \"test_loss\"\n",
    "    col2 = \"avg/loss\"\n",
    "    compare_correlations(df, base_name, intervention_name, col1, col2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in [\"fw_edu\", \"c4\", \"pile_uc\"]:\n",
    "    base_name = \"llama_416M_{dataset}_8.4BT\".format(dataset=dataset)\n",
    "    intervention_name = \"mamba_420M_{dataset}_8.4BT\".format(dataset=dataset)\n",
    "    col1 = \"test_loss\"\n",
    "    col2 = \"avg/loss\"\n",
    "    compare_correlations(df, base_name, intervention_name, col1, col2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in [\"fw_edu\", \"c4\", \"pile_uc\"]:\n",
    "    intervention_name = \"llama_416M_{dataset}_8.4BT\".format(dataset=dataset)\n",
    "    base_name = \"gpt2_420M_{dataset}_8.4BT\".format(dataset=dataset)\n",
    "    col1 = \"hellaswag/loss\"\n",
    "    col2 = \"piqa/loss\"\n",
    "    compare_correlations(df, base_name, intervention_name, col1, col2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_comparison_plots(\n",
    "    df, col1, col2, primary=\"arch\", secondary=\"dim\", tertiary=\"n_layers\", save_path=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates flexible comparison plots with any combination of grouping variables.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with the data\n",
    "        col1, col2: column names for x and y axes\n",
    "        primary: column to group separate plots by ('arch', 'dim', or 'n_layers')\n",
    "        secondary: column for subplots ('arch', 'dim', or 'n_layers')\n",
    "        tertiary: column for point shapes ('arch', 'dim', or 'n_layers')\n",
    "        save_path: optional path to save figures\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from scipy import stats\n",
    "\n",
    "    # Dynamic grouping values from data\n",
    "    group_values = {\n",
    "        \"arch\": sorted(df[\"arch\"].unique()),\n",
    "        \"dim\": sorted(df[\"dim\"].unique()),\n",
    "        \"n_layers\": sorted(df[\"n_layers\"].unique()),\n",
    "    }\n",
    "\n",
    "    # Verify valid grouping columns\n",
    "    for col in [primary, secondary, tertiary]:\n",
    "        if col not in group_values:\n",
    "            raise ValueError(\n",
    "                f\"Invalid grouping column: {col}. Must be one of {list(group_values.keys())}\"\n",
    "            )\n",
    "\n",
    "    colors = {\"c4\": \"blue\", \"pile_uc\": \"red\", \"fineweb_edu_100bt\": \"green\"}\n",
    "    markers = [\"o\", \"s\", \"^\", \"D\", \"v\", \"<\", \">\", \"p\", \"h\", \"8\"]\n",
    "    marker_dict = dict(\n",
    "        zip(group_values[tertiary], markers[: len(group_values[tertiary])])\n",
    "    )\n",
    "\n",
    "    for prim_val in group_values[primary]:\n",
    "        fig, axes = plt.subplots(\n",
    "            1,\n",
    "            len(group_values[secondary]),\n",
    "            figsize=(5 * len(group_values[secondary]), 5),\n",
    "        )\n",
    "        if len(group_values[secondary]) == 1:\n",
    "            axes = [axes]\n",
    "        fig.suptitle(f\"{primary}={prim_val}: {col2} vs {col1}\")\n",
    "\n",
    "        legend_elements = []\n",
    "\n",
    "        for sec_val, ax in zip(group_values[secondary], axes):\n",
    "            mask = (df[primary] == prim_val) & (df[secondary] == sec_val)\n",
    "            subset = df[mask]\n",
    "\n",
    "            for p_data in colors:\n",
    "                p_data_mask = subset[\"pretraining_data\"] == p_data\n",
    "                p_data_subset = subset[p_data_mask]\n",
    "\n",
    "                for tert_val in sorted(p_data_subset[tertiary].unique()):\n",
    "                    tert_mask = p_data_subset[tertiary] == tert_val\n",
    "                    data = p_data_subset[tert_mask]\n",
    "\n",
    "                    if len(data) > 0:\n",
    "                        scatter = ax.scatter(\n",
    "                            data[col1],\n",
    "                            data[col2],\n",
    "                            c=colors[p_data],\n",
    "                            marker=marker_dict[tert_val],\n",
    "                            alpha=0.6,\n",
    "                        )\n",
    "\n",
    "                        # Add to legend if new combination\n",
    "                        legend_key = (\n",
    "                            p_data,\n",
    "                            str(tert_val),\n",
    "                        )  # Convert to string for comparison\n",
    "                        if legend_key not in [\n",
    "                            (\n",
    "                                le.get_label().split(\", \")[0],\n",
    "                                le.get_label().split(\", \")[1].split()[0],\n",
    "                            )\n",
    "                            for le in legend_elements\n",
    "                        ]:\n",
    "                            scatter.set_label(f\"{p_data}, {tert_val} {tertiary}\")\n",
    "                            legend_elements.append(scatter)\n",
    "\n",
    "                if len(p_data_subset) > 1:\n",
    "                    slope, intercept, r_value, _, _ = stats.linregress(\n",
    "                        p_data_subset[col1], p_data_subset[col2]\n",
    "                    )\n",
    "                    x_range = np.linspace(\n",
    "                        p_data_subset[col1].min(), p_data_subset[col1].max(), 100\n",
    "                    )\n",
    "                    y_range = slope * x_range + intercept\n",
    "                    ax.plot(x_range, y_range, c=colors[p_data])\n",
    "\n",
    "                    mid_idx = len(x_range) // 2\n",
    "                    ax.annotate(\n",
    "                        f\"R² = {r_value**2:.2f}\",\n",
    "                        xy=(x_range[mid_idx], y_range[mid_idx]),\n",
    "                        xytext=(10, 10),\n",
    "                        textcoords=\"offset points\",\n",
    "                        color=colors[p_data],\n",
    "                        bbox=dict(facecolor=\"white\", edgecolor=\"none\", alpha=0.7),\n",
    "                    )\n",
    "\n",
    "            ax.set_title(f\"{secondary}={sec_val}\")\n",
    "            ax.set_xlabel(col1)\n",
    "            ax.set_ylabel(col2)\n",
    "\n",
    "        fig.legend(\n",
    "            handles=legend_elements, bbox_to_anchor=(1.05, 0.5), loc=\"center left\"\n",
    "        )\n",
    "        plt.tight_layout()\n",
    "\n",
    "        if save_path:\n",
    "            plt.savefig(f\"{save_path}_{prim_val}.png\", bbox_inches=\"tight\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Original way (by arch, then dim)\n",
    "# create_comparison_plots(df, 'test_loss', 'hellaswag/loss',\n",
    "#                        primary='arch', secondary='dim', tertiary='n_layers')\n",
    "\n",
    "# # By dimension, then architecture\n",
    "# create_comparison_plots(df, 'test_loss', 'hellaswag/loss',\n",
    "#                        primary='dim', secondary='arch', tertiary='n_layers')\n",
    "\n",
    "# # By n_layers, then dimension\n",
    "# create_comparison_plots(df, 'test_loss', 'hellaswag/loss',\n",
    "#                        primary='n_layers', secondary='dim', tertiary='arch')\n",
    "\n",
    "create_comparison_plots(\n",
    "    df,\n",
    "    \"hellaswag/loss\",\n",
    "    \"arc_challenge/loss\",\n",
    "    primary=\"dim\",\n",
    "    secondary=\"n_layers\",\n",
    "    tertiary=\"arch\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_grid_comparison(df, columns, max_checkpoints_per_group=100, save_path=None):\n",
    "    \"\"\"\n",
    "    Creates a grid of scatter plots comparing each column against others.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with the data\n",
    "        columns: list of column names to compare\n",
    "        max_checkpoints_per_group: maximum number of checkpoints to sample per arch-pretraining combination\n",
    "        save_path: optional path to save figure\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from scipy import stats\n",
    "    import numpy as np\n",
    "\n",
    "    # Sample checkpoints if needed\n",
    "    sampled_df = df.copy()\n",
    "    if max_checkpoints_per_group:\n",
    "        samples = []\n",
    "        for arch in df[\"arch\"].unique():\n",
    "            for p_data in df[\"pretraining_data\"].unique():\n",
    "                mask = (df[\"arch\"] == arch) & (df[\"pretraining_data\"] == p_data)\n",
    "                group_data = df[mask]\n",
    "                if len(group_data) > max_checkpoints_per_group:\n",
    "                    samples.append(\n",
    "                        group_data.sample(n=max_checkpoints_per_group, random_state=42)\n",
    "                    )\n",
    "                else:\n",
    "                    samples.append(group_data)\n",
    "        sampled_df = pd.concat(samples)\n",
    "\n",
    "    n = len(columns)\n",
    "    fig, axes = plt.subplots(n, n, figsize=(5 * n, 5 * n))\n",
    "\n",
    "    # Style settings\n",
    "    colors = {\"c4\": \"blue\", \"pile_uc\": \"red\", \"fineweb_edu_100bt\": \"green\"}\n",
    "    markers = {\"llama\": \"o\", \"mamba\": \"s\", \"gpt2\": \"^\"}\n",
    "\n",
    "    # Create plots\n",
    "    for i, col1 in enumerate(columns):\n",
    "        for j, col2 in enumerate(columns):\n",
    "            ax = axes[i, j]\n",
    "\n",
    "            if i != j:  # Skip diagonal\n",
    "                # Plot points for each architecture and pretraining data\n",
    "                for arch in sampled_df[\"arch\"].unique():\n",
    "                    arch_mask = sampled_df[\"arch\"] == arch\n",
    "                    arch_data = sampled_df[arch_mask]\n",
    "\n",
    "                    for p_data in colors:\n",
    "                        mask = arch_data[\"pretraining_data\"] == p_data\n",
    "                        data = arch_data[mask]\n",
    "\n",
    "                        if len(data) > 0:\n",
    "                            ax.scatter(\n",
    "                                data[col1],\n",
    "                                data[col2],\n",
    "                                c=colors[p_data],\n",
    "                                marker=markers[arch],\n",
    "                                alpha=0.6,\n",
    "                                label=f\"{arch}-{p_data}\",\n",
    "                            )\n",
    "\n",
    "                # Add one fit line for all data combined\n",
    "                if len(sampled_df) > 1:\n",
    "                    slope, intercept, r_value, _, _ = stats.linregress(\n",
    "                        sampled_df[col1], sampled_df[col2]\n",
    "                    )\n",
    "                    x_range = np.linspace(\n",
    "                        sampled_df[col1].min(), sampled_df[col1].max(), 100\n",
    "                    )\n",
    "                    y_range = slope * x_range + intercept\n",
    "                    # Use black color for the overall correlation line\n",
    "                    ax.plot(x_range, y_range, c=\"black\", linestyle=\"--\")\n",
    "\n",
    "                    # Add R² annotation\n",
    "                    mid_idx = len(x_range) // 2\n",
    "                    ax.annotate(\n",
    "                        f\"R² = {r_value**2:.2f}\",\n",
    "                        xy=(x_range[mid_idx], y_range[mid_idx]),\n",
    "                        xytext=(10, 10),\n",
    "                        textcoords=\"offset points\",\n",
    "                        color=\"black\",\n",
    "                        bbox=dict(facecolor=\"white\", edgecolor=\"none\", alpha=0.7),\n",
    "                    )\n",
    "\n",
    "                ax.set_xlabel(col1)\n",
    "                ax.set_ylabel(col2)\n",
    "                ax.grid(True, alpha=0.3)\n",
    "            else:\n",
    "                # Show column name in diagonal\n",
    "                ax.text(\n",
    "                    0.5,\n",
    "                    0.5,\n",
    "                    col1,\n",
    "                    horizontalalignment=\"center\",\n",
    "                    verticalalignment=\"center\",\n",
    "                    transform=ax.transAxes,\n",
    "                    fontsize=12,\n",
    "                )\n",
    "                ax.set_xticks([])\n",
    "                ax.set_yticks([])\n",
    "\n",
    "    # Add legend to the first subplot\n",
    "    handles, labels = axes[0, 1].get_legend_handles_labels()\n",
    "    unique_labels = []\n",
    "    unique_handles = []\n",
    "    for h, l in zip(handles, labels):\n",
    "        if l not in unique_labels:\n",
    "            unique_labels.append(l)\n",
    "            unique_handles.append(h)\n",
    "    fig.legend(\n",
    "        unique_handles, unique_labels, bbox_to_anchor=(1.02, 0.5), loc=\"center left\"\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, bbox_inches=\"tight\", dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# List of columns to compare\n",
    "columns_to_compare = [\n",
    "    \"arc_challenge/loss\",\n",
    "    \"arc_easy/loss\",\n",
    "    \"hellaswag/loss\",\n",
    "    \"piqa/loss\",\n",
    "    \"openbookqa/loss\",\n",
    "    \"winogrande/loss\",\n",
    "]\n",
    "# plot only for within_chinchilla = True\n",
    "df_within_chinchilla = df[df[\"within_chinchilla\"] == True]\n",
    "\n",
    "# Create the grid plot\n",
    "create_grid_comparison(\n",
    "    df_within_chinchilla,\n",
    "    columns_to_compare,\n",
    "    save_path=\"/lustre/fast/fast/pmayilvahanan/llm_line/code/llm_line/notebooks/grid_comparison.png\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
