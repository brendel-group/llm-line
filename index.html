<!doctype html>
<html lang="en">

<head>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <!-- Primary Meta Tags -->
    <title>LLMs on the Line: Data Determines Loss-To-Loss Scaling Laws</title>
    <meta name="title" content="LLMs on the Line: Data Determines Loss-To-Loss Scaling Laws">
    <meta name="description"
        content="Scaling laws guide the development of large language models (LLMs) by offering estimates for the optimal balance of model size, tokens, and compute. More recently, loss-to-loss scaling laws that relate losses across pretraining datasets and downstream tasks have emerged as a powerful tool for understanding and improving LLM performance. In this work, we investigate which factors most strongly influence loss-to-loss scaling. Our experiments reveal that the pretraining data and tokenizer determine the scaling trend. In contrast, model size, optimization hyperparameters, and even significant architectural differences, such as between transformer-based models like Llama and state-space models like Mamba, have limited impact. Consequently, practitioners should carefully curate suitable pretraining datasets for optimal downstream performance, while architectures and other settings can be freely optimized for training efficiency.">
    
    <!-- Open Graph / Facebook -->
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://brendel-group.github.io/llm-line">
    <meta property="og:title" content="LLMs on the Line: Data Determines Loss-To-Loss Scaling Laws">
    <meta property="og:description"
        content="Scaling laws guide the development of large language models (LLMs) by offering estimates for the optimal balance of model size, tokens, and compute. More recently, loss-to-loss scaling laws that relate losses across pretraining datasets and downstream tasks have emerged as a powerful tool for understanding and improving LLM performance. In this work, we investigate which factors most strongly influence loss-to-loss scaling. Our experiments reveal that the pretraining data and tokenizer determine the scaling trend. In contrast, model size, optimization hyperparameters, and even significant architectural differences, such as between transformer-based models like Llama and state-space models like Mamba, have limited impact. Consequently, practitioners should carefully curate suitable pretraining datasets for optimal downstream performance, while architectures and other settings can be freely optimized for training efficiency.">
    <meta property="og:image" content="https://brendel-group.github.io/llm-line/img/fig1.png">

    <!-- Twitter -->
    <meta property="twitter:card" content="summary_large_image">
    <meta property="twitter:url" content="https://brendel-group.github.io/llm-line">
    <meta property="twitter:title" content="LLMs on the Line: Data Determines Loss-To-Loss Scaling Laws">
    <meta property="twitter:description"
        content="Scaling laws guide the development of large language models (LLMs) by offering estimates for the optimal balance of model size, tokens, and compute. More recently, loss-to-loss scaling laws that relate losses across pretraining datasets and downstream tasks have emerged as a powerful tool for understanding and improving LLM performance. In this work, we investigate which factors most strongly influence loss-to-loss scaling. Our experiments reveal that the pretraining data and tokenizer determine the scaling trend. In contrast, model size, optimization hyperparameters, and even significant architectural differences, such as between transformer-based models like Llama and state-space models like Mamba, have limited impact. Consequently, practitioners should carefully curate suitable pretraining datasets for optimal downstream performance, while architectures and other settings can be freely optimized for training efficiency.">
    <meta property="twitter:image" content="https://brendel-group.github.io/llm-line/img/fig1.png">

    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/css/bootstrap.min.css"
        integrity="sha384-9aIt2nRpC12Uk9gS9baDl411NQApFmC26EwAOH8WgZl5MYYxFfc+NcPb1dKGj7Sk" crossorigin="anonymous">

    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Sans+Condensed&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Mono&display=swap" rel="stylesheet">

    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/Chart.js/2.9.3/Chart.min.js"></script>

    <style>
        .main {
            font-family: 'IBM Plex Sans Condensed', sans-serif;
        }

        .code {
            font-family: 'IBM Plex Mono', monospace;
        }

        h3 {
            margin-top: 1.0rem; 
        }

        .row-dense {
            padding-bottom: 0;
        }

        .a {
            color: gainsboro;
            font-family: 'IBM Plex Sans Condensed', sans-serif;
        }

        td {
            padding: 0 15px;
        }

        p {
            text-align: justify;

        }

        .collapse-container {
            text-align: center;
            position: relative;

        }

        .collapse-container #moreless.collapsed:after {
            content: '+ Show More';
        }

        .collapse-container #moreless:not(.collapsed):after {
            content: '- Show Less';
        }

        .collapse-container .collapse.collapse:not(.show) {
            display: block;
            /* height = lineheight * no of lines to display */
            height: 7.7em;
            overflow: hidden;
        }

        .collapse-container .collapse.collapse:not(.show):before {
            content: '';
            width: 100%;
            height: 7.7em;
            position: absolute;
            left: 0;
            top: 0;
            background: linear-gradient(rgba(255, 255, 255, 0), 60px, white);
        }

        .collapse-container .collapse.collapsing {
            height: 7.7em;
        }
    </style>

    <title>LLMs on the Line: Data Determines Loss-To-Loss Scaling Laws</title>
</head>

<body>
    <div class="container main">
        <div class="row">
            <div class="col-sm-2">
            </div>
            <div class="col-sm-8" id="main-content">
                <div class="row text-center my-5" id="#">
                    <h1>LLMs on the Line: Data Determines Loss-To-Loss Scaling Laws</h1>
                </div>

                <!-- Begin author list-->
                <div class="row text-center mb-4">
                    <div class="col-sm-4 mb-4">
                        Prasanna Mayilvahanan*
                        <a href="mailto:prasanna.mayilvahanan@gmail.com"><i class="far fa-envelope"></i></a>
                        <a href="https://x.com/prasannamayil"><i class="fab fa-x-twitter"></i></a><br>
                        MPI-IS, University of Tübingen, Tübingen AI Center
                    </div>
                    <div class="col-sm-4 mb-4">
                        Thaddäus Wiedemer*
                        <a href="mailto:thaddaeus.wiedemer@gmail.com"><i class="far fa-envelope"></i></a>
                        <a href="https://twitter.com/thwiedemer"><i class="fab fa-x-twitter"></i></a><br>
                        MPI-IS, University of Tübingen, Tübingen AI Center
                    </div>
                    <div class="col-sm-4 mb-4">
                        Sayak Mallick
                        <a href="mailto:sayak.mallick@gmail.com"><i class="far fa-envelope"></i></a>
                        <a href="https://x.com/prasannamayil"><i class="fab fa-x-twitter"></i></a><br>
                        University of Tübingen, MPI-IS, Tübingen AI Center
                    </div>
                    <div class="col-sm-2  mb-2">
                    </div>
                    <div class="col-sm-4 mb-4">
                        Matthias Bethge
                        <a href="mailto:matthias@bethgelab.org"><i class="far fa-envelope"></i></a><br>
                        University of Tübingen, Tübingen AI Center
                    </div>
                    <div class="col-sm-4 mb-4">
                        Wieland Brendel
                        <a href="mailto:wieland.brendel@tue.mpg.de"><i class="far fa-envelope"></i></a><br>
                        MPI-IS, ELLIS Institute Tübingen, Tübingen AI Center
                    </div>
                </div>
                <!-- End author list-->

                <div class="row text-center">
                    <div class="col-sm-4 mb-4 offset-sm-2">
                        <h4>
                            <a href="https://arxiv.org/abs/xxxxxxxx" target="_blank">
                                <i class="fas fa-file-alt"></i>
                                Paper
                            </a>
                        </h4>
                    </div>
                    <div class="col-sm-4 mb-4">
                        <h4>
                            <a href="https://github.com/brendel-group/llm-line" target="_blank">
                                <i class="fab fa-github"></i>
                                Code
                            </a>
                        </h4>
                    </div>
                </div>

                <div class="row text-center">
                    <p>
                        <b>tl;dr:</b>
                        <span class="text-muted">
                            We find that loss-to-loss scaling laws depend on pretraining data and tokenizer, not model size, optimization, or even significant architectural differences.
                        </span>
                    </p>
                </div>

                <div class="row mt-2">
                    <h3>News</h3>
                </div>

                <div class="row">
                    <table>
                        <tr>
                            <td>
                                <span class="badge badge-pill badge-primary">Feb '25</span>
                            </td>
                            <td>
                                The pre-print is now available on <a href="https://arxiv.org/abs/xxxxx" target="_blank">arXiv</a>.
                            </td>
                        </tr>
                    </table>
                </div>

                <div class="row mt-2">
                    <div class="col-12">
                        <p>
                        </p>
                    </div>
                </div>

                <div class="row mt-2">
                    <h3>Abstract</h3>
                </div>
                <div class="row">
                    <p>
                        Scaling laws guide the development of large language models (LLMs) by offering estimates for the optimal balance of model size, tokens, and compute. More recently, loss-to-loss scaling laws that relate losses across pretraining datasets and downstream tasks have emerged as a powerful tool for understanding and improving LLM performance. In this work, we investigate which factors most strongly influence loss-to-loss scaling. Our experiments reveal that the pretraining data and tokenizer determine the scaling trend. In contrast, model size, optimization hyperparameters, and even significant architectural differences, such as between transformer-based models like Llama and state-space models like Mamba, have limited impact. Consequently, practitioners should carefully curate suitable pretraining datasets for optimal downstream performance, while architectures and other settings can be freely optimized for training efficiency.
                    </p>
                </div>

                <div class="row mt-2">
                    <h3>Overview</h3>
                </div>
                <div class="row mt-2 row-dense">
                    <div class="col-12">
                        <p>
                            We make three main observations (illustrated in the figure below):
                        </p>
                        <ol>
                            <li>LLMs' loss-to-loss scaling consistently follows shifted power laws.</li>
                            <br>
                            <li>Pretraining data and tokenizer are the most salient factors for these scaling laws.</li>
                            <br>
                            <li>In contrast, architecture plays a minor role, while model size, context length, and optimizer settings have negligible impact on loss-to-loss scalin trends.</li>
                        </ol>
                        <div class="d-flex justify-content-center">
                            <img src="img/fig1.png" class="img-fluid" style="max-width: 100%;"/>
                        </div>
                    </div>
                </div>

                <div class="row mt-2">
                    <h3>What did we essentially do and how?</h3>
                </div>
                <div class="row mt-2 row-dense">
                    <div class="col-12">
                        <p>
                            Our study investigates how different design choices impact loss-to-loss scaling laws in LLMs. Using over 6,000 model configurations, we conduct controlled interventions by varying factors such as pretraining data, tokenizer, architecture, model size, and optimization settings. By analyzing the resulting changes in loss-to-loss scaling laws, we uncover key insights into what truly drives them. 
                        </p>
                    </div>
                </div>


                <div class="row mt-2">
                    <h3>What does this imply?</h3>
                </div>
                <div class="row mt-2 row-dense">
                    <div class="col-12">
                        <p>
                            Traditionally, LLM development has placed heavy emphasis on architectural improvements and model scaling strategies. However, our research demonstrates that even drastically different architectures, such as LLaMA (a transformer-based model) and Mamba (a state-space model), exhibit nearly identical loss-to-loss scaling when trained on the same data. Meanwhile, changing the pretraining dataset leads to significant shifts in the scaling behavior, meaning that dataset curation is a far more influential lever for improving downstream performance. These insights suggest that practitioners should rethink their optimization strategies—prioritizing high-quality, well-curated datasets if they want to improve downstream performance.
                        </p>
                    </div>
                </div>
                <div class="row">
                    <h3>Acknowledgements & Funding</h3>
                </div>
                <div class="row mt-2">
                    <p>
                        We would like to thank (in alphabetical order) Ameya Prabhu, Attila Juhos, Evgenia Rusak, Fanfei Li, Jack Brady, Thomas Klein, and Vishaal Udandarao for helpful discussions and feedback. 
                        
                        We thank the <a href="https://imprs.is.mpg.de/" target="_blank">International Max Planck Research School for Intelligent Systems (IMPRS-IS)</a> for supporting PM and TW. This work was supported by the German Federal Ministry of Education and Research (BMBF): <a href="https://tuebingen.ai" target="_blank">Tübingen AI Center, FKZ: 01IS18039A</a>. WB acknowledges financial support via an Emmy Noether Grant funded by the German Research Foundation (DFG) under grant no. BR 6382/1-1 and via the Open Philantropy Foundation funded by the Good Ventures Foundation. WB is a member of the <a href="https://uni-tuebingen.de/en/research/core-research/cluster-of-excellence-machine-learning/home/" target="_blank">Machine Learning Cluster of Excellence, EXC number 2064/1 – Project number 390727645</a>. This research utilized compute resources at the Tübingen Machine Learning Cloud, DFG FKZ INST 37/1057-1 FUGG.
                    </p>
                </div>
                <div class="row">
                    <h3>BibTeX</h3>
                </div>
                <div class="row">
                    <p>If you find our study helpful, please cite our paper:</p>
                </div>
                <div class="row justify-content-md-center">
                    <div class="col-sm-12 rounded p-3 m-2" style="background-color:lightgray;">
                        <small class="code">
                            @inproceedings{mayilvahanan2025llmsline,<br>
                                &nbsp;&nbsp;title={LLMs on the Line: Data Determines Loss-To-Loss Scaling Laws},<br>
                                &nbsp;&nbsp;author={<br>
                                &nbsp;&nbsp;&nbsp;&nbsp;Prasanna Mayilvahanan and Thaddäus Wiedemer and Sayak Mallick and Matthias Bethge and Wieland Brendel<br>
                                &nbsp;&nbsp;},<br>
                                &nbsp;&nbsp;booktitle={xxxx},<br>
                                &nbsp;&nbsp;year={2024},<br>
                                &nbsp;&nbsp;url={xxxx}<br>
                            }
                        </small>
                    </div>
                </div>

                <div class="row">
                    <small class="text-muted">Webpage designed using Bootstrap 4.5 following a layout by <a href="https://rzimmermann.com/" target="_blank">Roland Zimmermann</a>.</small>
                    <a href="#" class="ml-auto"><i class="fas fa-sort-up"></i></a>
                </div>

            </div>
        </div>

    </div>

    <!-- Optional JavaScript -->
    <!-- jQuery first, then Popper.js, then Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"
        integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
        crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
        integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
        crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js"
        integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI"
        crossorigin="anonymous"></script>

    <script>
        $(function () {
            $('[data-toggle="tooltip"]').tooltip()
        })
    </script>

</body>

</html>
