<!doctype html>
<html lang="en">

<head>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <!-- Primary Meta Tags -->
    <title>LLMs on the Line: Data Determines Loss-To-Loss Scaling Laws</title>
    <meta name="title" content="LLMs on the Line: Data Determines Loss-To-Loss Scaling Laws">
    <meta name="description"
        content="Scaling laws guide the development of large language models (LLMs) by offering estimates for the optimal balance of model size, tokens, and compute. More recently, loss-to-loss scaling laws that relate losses across pretraining datasets and downstream tasks have emerged as a powerful tool for understanding and improving LLM performance. In this work, we investigate which factors most strongly influence loss-to-loss scaling. Our experiments reveal that the pretraining data and tokenizer determine the scaling trend. In contrast, model size, optimization hyperparameters, and even significant architectural differences, such as between transformer-based models like Llama and state-space models like Mamba, have limited impact. Consequently, practitioners should carefully curate suitable pretraining datasets for optimal downstream performance, while architectures and other settings can be freely optimized for training efficiency.">
    
    <!-- Open Graph / Facebook -->
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://brendel-group.github.io/llm-line">
    <meta property="og:title" content="LLMs on the Line: Data Determines Loss-To-Loss Scaling Laws">
    <meta property="og:description"
        content="Scaling laws guide the development of large language models (LLMs) by offering estimates for the optimal balance of model size, tokens, and compute. More recently, loss-to-loss scaling laws that relate losses across pretraining datasets and downstream tasks have emerged as a powerful tool for understanding and improving LLM performance. In this work, we investigate which factors most strongly influence loss-to-loss scaling. Our experiments reveal that the pretraining data and tokenizer determine the scaling trend. In contrast, model size, optimization hyperparameters, and even significant architectural differences, such as between transformer-based models like Llama and state-space models like Mamba, have limited impact. Consequently, practitioners should carefully curate suitable pretraining datasets for optimal downstream performance, while architectures and other settings can be freely optimized for training efficiency.">
    <meta property="og:image" content="https://brendel-group.github.io/llm-line/img/fig1.png">

    <!-- Twitter -->
    <meta property="twitter:card" content="summary_large_image">
    <meta property="twitter:url" content="https://brendel-group.github.io/llm-line">
    <meta property="twitter:title" content="LLMs on the Line: Data Determines Loss-To-Loss Scaling Laws">
    <meta property="twitter:description"
        content="Scaling laws guide the development of large language models (LLMs) by offering estimates for the optimal balance of model size, tokens, and compute. More recently, loss-to-loss scaling laws that relate losses across pretraining datasets and downstream tasks have emerged as a powerful tool for understanding and improving LLM performance. In this work, we investigate which factors most strongly influence loss-to-loss scaling. Our experiments reveal that the pretraining data and tokenizer determine the scaling trend. In contrast, model size, optimization hyperparameters, and even significant architectural differences, such as between transformer-based models like Llama and state-space models like Mamba, have limited impact. Consequently, practitioners should carefully curate suitable pretraining datasets for optimal downstream performance, while architectures and other settings can be freely optimized for training efficiency.">
    <meta property="twitter:image" content="https://brendel-group.github.io/llm-line/img/fig1.png">

    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/css/bootstrap.min.css"
        integrity="sha384-9aIt2nRpC12Uk9gS9baDl411NQApFmC26EwAOH8WgZl5MYYxFfc+NcPb1dKGj7Sk" crossorigin="anonymous">

    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Sans+Condensed&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Mono&display=swap" rel="stylesheet">

    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/Chart.js/2.9.3/Chart.min.js"></script>

    <style>
        .main {
            font-family: 'IBM Plex Sans Condensed', sans-serif;
        }

        .code {
            font-family: 'IBM Plex Mono', monospace;
        }

        h3 {
            margin-top: 1.0rem; 
        }

        .row-dense {
            padding-bottom: 0;
        }

        .a {
            color: gainsboro;
            font-family: 'IBM Plex Sans Condensed', sans-serif;
        }

        td {
            padding: 0 15px;
        }

        p {
            text-align: justify;

        }

        .collapse-container {
            text-align: center;
            position: relative;

        }

        .collapse-container #moreless.collapsed:after {
            content: '+ Show More';
        }

        .collapse-container #moreless:not(.collapsed):after {
            content: '- Show Less';
        }

        .collapse-container .collapse.collapse:not(.show) {
            display: block;
            /* height = lineheight * no of lines to display */
            height: 7.7em;
            overflow: hidden;
        }

        .collapse-container .collapse.collapse:not(.show):before {
            content: '';
            width: 100%;
            height: 7.7em;
            position: absolute;
            left: 0;
            top: 0;
            background: linear-gradient(rgba(255, 255, 255, 0), 60px, white);
        }

        .collapse-container .collapse.collapsing {
            height: 7.7em;
        }
    </style>

    <title>LLMs on the Line: Data Determines Loss-To-Loss Scaling Laws</title>
</head>

<body>
    <div class="container main">
        <div class="row">
            <div class="col-sm-2">
            </div>
            <div class="col-sm-8" id="main-content">
                <div class="row text-center my-5" id="#">
                    <h1>LLMs on the Line: Data Determines Loss-To-Loss Scaling Laws</h1>
                </div>

                <!-- Begin author list-->
                <div class="row text-center mb-4">
                    <div class="col-sm-4 mb-4">
                        Prasanna Mayilvahanan*
                        <a href="mailto:attila.juhos@tuebingen.mpg.de"><i class="far fa-envelope"></i></a>
                        <a href="https://twitter.com/JuhosAttila"><i class="fab fa-x-twitter"></i></a><br>
                        MPI-IS, University of Tübingen, Tübingen AI Center
                    </div>
                    <div class="col-sm-4 mb-4">
                        Thaddäus Wiedemer*
                        <a href="mailto:thaddaeus.wiedemer@gmail.com"><i class="far fa-envelope"></i></a>
                        <a href="https://twitter.com/thwiedemer"><i class="fab fa-x-twitter"></i></a><br>
                        MPI-IS, University of Tübingen, Tübingen AI Center
                    </div>
                    <div class="col-sm-4 mb-4">
                        Sayak Mallick*
                        <a href="mailto:"><i class="far fa-envelope"></i></a>
                        <a href="https://twitter.com/kotekjedi_ml"><i class="fab fa-x-twitter"></i></a><br>
                        University of Tübingen, MPI-IS, TÜbingen AI Center
                    </div>
                    <div class="col-sm-4 mb-4">
                        Matthias Bethge
                        <a href="mailto:matthias@bethgelab.org"><i class="far fa-envelope"></i></a><br>
                        University of Tübingen, Tübingen AI Center
                    </div>
                    <div class="col-sm-4 mb-4">
                        Wieland Brendel
                        <a href="mailto:wieland.brendel@tue.mpg.de"><i class="far fa-envelope"></i></a><br>
                        MPI-IS, ELLIS Institute Tübingen, Tübingen AI Center
                    </div>
                </div>
                <!-- End author list-->

                <div class="row text-center">
                    <div class="col-sm-4 mb-4 offset-sm-2">
                        <h4>
                            <a href="https://arxiv.org/abs/xxxxxxxx" target="_blank">
                                <i class="fas fa-file-alt"></i>
                                Paper
                            </a>
                        </h4>
                    </div>
                    <div class="col-sm-4 mb-4">
                        <h4>
                            <a href="https://github.com/brendel-group/llm-line" target="_blank">
                                <i class="fab fa-github"></i>
                                Code
                            </a>
                        </h4>
                    </div>
                </div>

                <div class="row text-center">
                    <p>
                        <b>tl;dr:</b>
                        <span class="text-muted">
                            We investigate which factors most strongly influence loss-to-loss scaling.
                        </span>
                    </p>
                </div>

                <div class="row mt-2">
                    <h3>News</h3>
                </div>

                <div class="row">
                    <table>
                        <tr>
                            <td>
                                <span class="badge badge-pill badge-primary">Feb '24</span>
                            </td>
                            <td>
                                Our paper was xxxxxx!
                            </td>
                        </tr>
                        <tr>
                            <td>
                                <span class="badge badge-pill badge-primary">Oct '23</span>
                            </td>
                            <td>
                                The pre-print is now available on <a href="https://arxiv.org/abs/xxxxx" target="_blank">arXiv</a>.
                            </td>
                        </tr>
                    </table>
                </div>

                <div class="row mt-2">
                    <div class="col-12">
                        <p>
                        </p>
                    </div>
                </div>

                <div class="row mt-2">
                    <h3>Abstract</h3>
                </div>
                <div class="row mt-2">
                    <div class="col-12 collapse-container">
                        <p class="collapse" id="abstractText" aria-expanded="false">
                            Scaling laws guide the development of large language models (LLMs) by offering estimates for the optimal balance of model size, tokens, and compute. More recently, loss-to-loss scaling laws that relate losses across pretraining datasets and downstream tasks have emerged as a powerful tool for understanding and improving LLM performance. In this work, we investigate which factors most strongly influence loss-to-loss scaling. Our experiments reveal that the pretraining data and tokenizer determine the scaling trend. In contrast, model size, optimization hyperparameters, and even significant architectural differences, such as between transformer-based models like Llama and state-space models like Mamba, have limited impact. Consequently, practitioners should carefully curate suitable pretraining datasets for optimal downstream performance, while architectures and other settings can be freely optimized for training efficiency.
                        </p>
                       <a role="button" id="moreless" class="collapsed" data-toggle="collapse" href="#abstractText" aria-expanded="false" aria-controls="abstractText"></a>
                    </div>
                </div>

                <div class="row mt-2">
                    <h3>Overview</h3>
                </div>
                <div class="row mt-2 row-dense">
                    <div class="col-12">
                        <p>
                            We make three main observations, illustrated in Fig. 1:
                                1. LLMs’ loss-to-loss scaling consistently follows shifted power laws.
                                2. Pretraining data and tokenizer are the most salient factors for these scaling laws.
                                3. In contrast, architecture plays a minor role, while model size, context length, and optimizer settings have negligible impact on loss-to-loss scaling.
                        </p>
                        <p>   
                            Further, we put our observations in the context of downstream scaling laws and discuss the relationship between loss-to-loss and compute-to-loss scaling laws. Our results indicate that different LLM architectures might encode very similar inductive biases, freeing practitioners to optimize architectures for training efficiency without adversely affecting downstream scaling laws.
                        </p>
                        <div class="d-flex justify-content-center">
                            <img src="img/fig1.png" class="img-fluid" style="max-width: 100%;"/>
                        </div>
                        <small class="text-muted">
                            <p>
                                xxxx
                            </p>
                        </small>
                    </div>
                </div>

                <div class="row mt-2">
                    <h3>What did we essentially do and how?</h3>
                </div>
                <div class="row mt-2 row-dense">
                    <div class="col-12">
                        <p>
                            Our study investigates how different design choices impact loss-to-loss scaling laws in LLMs. Using over 6,000 model checkpoints, we conduct controlled interventions by varying factors such as pretraining data, tokenizer, architecture, model size, and optimization settings. By analyzing the resulting changes in train-to-test loss scaling, we uncover key insights into what truly drives downstream performance. Contrary to expectations, we find that pretraining dataset composition has the most significant impact, while even major architectural differences—such as switching from transformers to state-space models—yield minimal changes in scaling behavior. These findings suggest that rather than focusing on new model architectures, researchers and practitioners should prioritize refining pretraining data selection to achieve optimal performance.
                        </p>
                        <div class="d-flex justify-content-center">
                            <img src="img/fig2.png" class="img-fluid" style="max-width: 100%;"/>
                        </div>
                        <ol>
                            xxxx    
                        </ol>
                    </div>
                </div>


                <div class="row mt-2">
                    <h3>What does this look like in practice?</h3>
                </div>
                <div class="row mt-2 row-dense">
                    <div class="col-12">
                        <p>
                            Traditionally, LLM development has placed heavy emphasis on architectural improvements and model scaling strategies. However, our research demonstrates that even drastically different architectures, such as LLaMA (a transformer-based model) and Mamba (a state-space model), exhibit nearly identical loss-to-loss scaling when trained on the same data. Meanwhile, changing the pretraining dataset leads to significant shifts in the scaling behavior, meaning that dataset curation is a far more influential lever for improving downstream performance. These insights suggest that practitioners should rethink their optimization strategies—prioritizing high-quality, well-curated datasets over iterative model modifications.
                        </p>
                        <div class="d-flex justify-content-center">
                            <img src="img/fig4.png" class="img-fluid" style="max-width: 100%;"/>
                        </div>
                        <small class="text-muted">
                            xxxx
                        </small>
                        <div class="d-flex justify-content-center">
                            <img src="img/fig4.png" class="img-fluid" style="max-width: 100%;"/>
                        </div>
                        <small class="text-muted">
                             xxxx
                        </small>
                    </div>
                </div>


                <div class="row mt-2">
                    <h3>What does this imply?</h3>
                </div>
                <div class="row mt-2 row-dense">
                    <div class="col-12">
                        <p>
                            Our results challenge the conventional wisdom that architecture is the key determinant of LLM performance. Instead, they indicate that models trained on the same datasets tend to converge toward similar scaling behaviors, regardless of whether they are transformer-based or use alternative architectures. This raises fundamental questions about the role of inductive biases in different model designs and suggests that most performance gains may come from better data selection rather than novel architectures. Additionally, our findings imply that computational resources can be allocated more efficiently by focusing on dataset improvements rather than model complexity. Ultimately, this shift in perspective has broad implications for both academia and industry, suggesting that future advancements in LLMs may depend more on data curation than on architectural innovation.
                        </p>
                        <div class="d-flex justify-content-center">
                            <img src="img/fig6.png" class="img-fluid" style="max-width: 100%;"/>
                        </div>
                        <small class="text-muted">
                            xxxx
                        </small>
                    </div>
                </div>

                <div class="row">
                    <h3>Acknowledgements & Funding</h3>
                </div>
                <div class="row mt-2">
                    <div class="col-12 collapse-container">
                        <p class="collapse" id="acknowledgmentsText" aria-expanded="false">
                            We thank (in alphabetical order): xxxxxx
                            The authors thank the <a href="https://imprs.is.mpg.de/" target="_blank">International Max Planck Research School for Intelligent Systems (IMPRS-IS)</a> for supporting TW and AJ.
                        </p>
                       <a role="button" id="moreless" class="collapsed" data-toggle="collapse" href="#acknowledgmentsText" aria-expanded="false" aria-controls="acknowledgmentsText"></a>
                    </div>
                </div>
                <div class="row">
                    <h3>BibTeX</h3>
                </div>
                <div class="row">
                    <p>If you find our study helpful, please cite our paper:</p>
                </div>
                <div class="row justify-content-md-center">
                    <div class="col-sm-12 rounded p-3 m-2" style="background-color:lightgray;">
                        <small class="code">
                            @inproceedings{wiedemer2024provable,<br>
                                &nbsp;&nbsp;title={xxxxx},<br>
                                &nbsp;&nbsp;author={<br>
                                &nbsp;&nbsp;&nbsp;&nbsp;Thadd{\"a}us Wiedemer and Jack Brady and Alexander Panfilov and Attila Juhos and Matthias Bethge and Wieland Brendel<br>
                                &nbsp;&nbsp;},<br>
                                &nbsp;&nbsp;booktitle={xxxx},<br>
                                &nbsp;&nbsp;year={2024},<br>
                                &nbsp;&nbsp;url={xxxx}<br>
                            }
                        </small>
                    </div>
                </div>

                <div class="row">
                    <small class="text-muted">Webpage designed using Bootstrap 4.5 following a layout by <a href="https://rzimmermann.com/" target="_blank">Roland Zimmermann</a>.</small>
                    <a href="#" class="ml-auto"><i class="fas fa-sort-up"></i></a>
                </div>

            </div>
        </div>

    </div>

    <!-- Optional JavaScript -->
    <!-- jQuery first, then Popper.js, then Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"
        integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
        crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
        integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
        crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js"
        integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI"
        crossorigin="anonymous"></script>

    <script>
        $(function () {
            $('[data-toggle="tooltip"]').tooltip()
        })
    </script>

</body>

</html>

</html>

